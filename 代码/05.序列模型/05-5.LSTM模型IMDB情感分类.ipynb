{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b75c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.datasets import IMDB\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tarfile\n",
    "import time\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35417fc",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8714e42c-a563-443d-909e-30c771dbc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB（Internet Movie Database）是一个来自互联网的电影数据库，\n",
    "# 其中包含了50000条严重两极分化的电影评论。\n",
    "# 数据集被划分为训练集和测试集，其中训练集和测试集中各有25000条评论，\n",
    "# 并且训练集和测试集都包含50%的正面评论和50%的消极评论。\n",
    "# 数据集下载地址：http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f33d7df-f30f-420f-acdb-251ffe75d375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c30ef7c7d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动设置CPU生成随机数的种子，方便下次复现实验结果\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2516910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data\"\n",
    "batch_size = 10\n",
    "# 检查是否有GPU可用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a2b705-dbf7-49e4-888a-4e8f8e8ed57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imdb(folder='train', data_root=DATA_ROOT+\"/aclImdb\"):  \n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([label, review])\n",
    "    random.shuffle(data)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324d3fcb-3d60-4ce5-9512-41ff0b21362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5723.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5619.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:01<00:00, 6577.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:01<00:00, 6440.26it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = read_imdb('train'), read_imdb('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba4598-8e6a-46c8-8190-e959896ecea7",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2df529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab623e7-5b34-48d0-9d47-2bb30b2685e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'the', 'an', 'example', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('here is the an example!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56d88b0-b4ba-4b14-b915-08b69d0a6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词迭代器\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a30a67-82ac-4146-ac32-2f109808e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用分词迭代器构建词汇表\n",
    "# specials参数用于自定义词表：\"<pad>\"和\"<unk>\"，分别表示占位符和未登录词（没有被收录在词表中的词）\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<pad>\", \"<unk>\"])\n",
    "# 未在词汇表的数据的索引被设置为词汇表中\"<unk>\"的索引。\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fce592-5eec-4ed5-b5bd-9298f68297c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理pipeline\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 1 if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47447cf-e744-43f3-baaa-6931150acca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132, 10, 41, 465, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(text_pipeline('here is an example <pad> <pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2017bef-258c-4a9a-a658-55a548deca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(label_pipeline('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c310a7-d195-4d8a-a65c-1db33c6ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = text_pipeline('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b3f9d1-a045-48a5-b6c3-4e62a2c157cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理batch数据，包括对变长数据的处理等\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = text_pipeline(_text)[:max_length]\n",
    "         length_list.append(len(processed_text))\n",
    "         text_list.append((processed_text + pad * max_length)[:max_length])\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.tensor(text_list, dtype=torch.int64)\n",
    "    length_list = torch.tensor(length_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), length_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad4d9d37-4343-4448-ab56-4c2b1ea86d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用to_map_style_dataset函数将迭代器转化为Dataset类型\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_eval_ = random_split(train_dataset, \n",
    "                                         [num_train, len(train_dataset) - num_train])\n",
    "train_dataloader = DataLoader(split_train_, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "eval_dataloader = DataLoader(split_eval_, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd2c24",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ccc142-8608-4e14-8a50-6e8ee41f6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional,\n",
    "                 dropout_rate, pad_index=0, pretrained_embedding=None):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=bidirectional,\n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.init_weights(pretrained_embedding)\n",
    "\n",
    "    def init_weights(self, pretrained_embedding):\n",
    "        initrange = 0.5\n",
    "        if pretrained_embedding != None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embedding)\n",
    "            # 直接加载预训练好的weight, 所以不需要更新它\n",
    "            self.embedding.weight.requires_grad = False \n",
    "        else:\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            self.embedding.weight.requires_grad = True \n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, ids, length):\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        packed_embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, length.to(\"cpu\"), batch_first=True, \n",
    "                                                            enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        output, output_length = torch.nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        prediction = self.fc(hidden)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf8b363-758f-481d-83fe-45941667d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于情感分类的训练数据集不是很⼤，\n",
    "# 为应对过拟合，可以使⽤在更⼤规模语料上预训练的词向量作为每个词的特征向量。\n",
    "# 这⾥，我们为词典vocab中的每个词加载300维的GloVe词向量。\n",
    "# 注意，预训练词向量的维度需要与创建的模型中的嵌⼊层维度embedding_dim⼀致。\n",
    "# 第⼀次加载预训练词向量实例时会⾃动下载相应的词向量到cache参数指定的⽂件夹（默认为.vector_cache）。\n",
    "glove_vocab = Vocab.GloVe(name='6B', dim=300, cache=\"./model/glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b897cea-8129-43e2-ac8a-635fe6e59a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后，我们将⽤这些词向量作为评论中每个词的特征向量。\n",
    "# 此外，在训练中我们不再更新这些词向量。\n",
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count += 0\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\")\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72b7b3be-45cc-454b-90f6-c9c169a8b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = load_pretrained_embedding(vocab.get_itos(), glove_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f534a3ce-087a-4001-b81e-596fdbbcb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "# 由于数据的情感极性共分为两类，因此这里我们要把output_dim的值设置为2。\n",
    "output_dim = 2\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_rate, pretrained_embedding = pretrained_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc0e08",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42621c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e168c5-e2d9-4da4-9d28-804ffc28a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "639acdf9-1033-415a-8992-7965a50492fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "    for idx, (label, ids, length) in enumerate(dataloader):\n",
    "        label = label.to(device)\n",
    "        ids = ids.to(device)\n",
    "        length = length.to(device)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label) # loss计算\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        # 梯度更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(accuracy.item())\n",
    "        total_acc += (prediction.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc / total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return train_losses, train_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a51c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, criterion, device):\n",
    "    model = model.to(device)\n",
    "    # 切换到推理模式\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, ids, length) in enumerate(dataloader):\n",
    "            label = label.to(device)\n",
    "            ids = ids.to(device)\n",
    "            length = length.to(device)\n",
    "            prediction = model(ids, length)\n",
    "            loss = criterion(prediction, label) # loss计算\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            test_losses.append(loss.item())\n",
    "            test_accs.append(accuracy.item())\n",
    "            total_acc += (prediction.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                      '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                                  total_acc / total_count))\n",
    "                total_acc, total_count = 0, 0\n",
    "                start_time = time.time()\n",
    "    return test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f2fd26c-a4e6-45f8-803a-2d3e20534659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    50/ 2375 batches | accuracy    0.527\n",
      "| epoch   0 |   100/ 2375 batches | accuracy    0.530\n",
      "| epoch   0 |   150/ 2375 batches | accuracy    0.546\n",
      "| epoch   0 |   200/ 2375 batches | accuracy    0.558\n",
      "| epoch   0 |   250/ 2375 batches | accuracy    0.606\n",
      "| epoch   0 |   300/ 2375 batches | accuracy    0.594\n",
      "| epoch   0 |   350/ 2375 batches | accuracy    0.530\n",
      "| epoch   0 |   400/ 2375 batches | accuracy    0.610\n",
      "| epoch   0 |   450/ 2375 batches | accuracy    0.600\n",
      "| epoch   0 |   500/ 2375 batches | accuracy    0.606\n",
      "| epoch   0 |   550/ 2375 batches | accuracy    0.538\n",
      "| epoch   0 |   600/ 2375 batches | accuracy    0.618\n",
      "| epoch   0 |   650/ 2375 batches | accuracy    0.616\n",
      "| epoch   0 |   700/ 2375 batches | accuracy    0.682\n",
      "| epoch   0 |   750/ 2375 batches | accuracy    0.584\n",
      "| epoch   0 |   800/ 2375 batches | accuracy    0.664\n",
      "| epoch   0 |   850/ 2375 batches | accuracy    0.640\n",
      "| epoch   0 |   900/ 2375 batches | accuracy    0.662\n",
      "| epoch   0 |   950/ 2375 batches | accuracy    0.684\n",
      "| epoch   0 |  1000/ 2375 batches | accuracy    0.654\n",
      "| epoch   0 |  1050/ 2375 batches | accuracy    0.706\n",
      "| epoch   0 |  1100/ 2375 batches | accuracy    0.644\n",
      "| epoch   0 |  1150/ 2375 batches | accuracy    0.666\n",
      "| epoch   0 |  1200/ 2375 batches | accuracy    0.714\n",
      "| epoch   0 |  1250/ 2375 batches | accuracy    0.700\n",
      "| epoch   0 |  1300/ 2375 batches | accuracy    0.706\n",
      "| epoch   0 |  1350/ 2375 batches | accuracy    0.704\n",
      "| epoch   0 |  1400/ 2375 batches | accuracy    0.650\n",
      "| epoch   0 |  1450/ 2375 batches | accuracy    0.742\n",
      "| epoch   0 |  1500/ 2375 batches | accuracy    0.700\n",
      "| epoch   0 |  1550/ 2375 batches | accuracy    0.694\n",
      "| epoch   0 |  1600/ 2375 batches | accuracy    0.714\n",
      "| epoch   0 |  1650/ 2375 batches | accuracy    0.712\n",
      "| epoch   0 |  1700/ 2375 batches | accuracy    0.748\n",
      "| epoch   0 |  1750/ 2375 batches | accuracy    0.714\n",
      "| epoch   0 |  1800/ 2375 batches | accuracy    0.778\n",
      "| epoch   0 |  1850/ 2375 batches | accuracy    0.676\n",
      "| epoch   0 |  1900/ 2375 batches | accuracy    0.750\n",
      "| epoch   0 |  1950/ 2375 batches | accuracy    0.756\n",
      "| epoch   0 |  2000/ 2375 batches | accuracy    0.730\n",
      "| epoch   0 |  2050/ 2375 batches | accuracy    0.726\n",
      "| epoch   0 |  2100/ 2375 batches | accuracy    0.738\n",
      "| epoch   0 |  2150/ 2375 batches | accuracy    0.730\n",
      "| epoch   0 |  2200/ 2375 batches | accuracy    0.754\n",
      "| epoch   0 |  2250/ 2375 batches | accuracy    0.698\n",
      "| epoch   0 |  2300/ 2375 batches | accuracy    0.696\n",
      "| epoch   0 |  2350/ 2375 batches | accuracy    0.664\n",
      "| epoch   0 |    50/  125 batches | accuracy    0.765\n",
      "| epoch   0 |   100/  125 batches | accuracy    0.800\n",
      "epoch: 1\n",
      "train_loss: 0.609, train_acc: 0.666\n",
      "eval_loss: 0.455, eval_acc: 0.786\n",
      "| epoch   1 |    50/ 2375 batches | accuracy    0.747\n",
      "| epoch   1 |   100/ 2375 batches | accuracy    0.690\n",
      "| epoch   1 |   150/ 2375 batches | accuracy    0.698\n",
      "| epoch   1 |   200/ 2375 batches | accuracy    0.668\n",
      "| epoch   1 |   250/ 2375 batches | accuracy    0.666\n",
      "| epoch   1 |   300/ 2375 batches | accuracy    0.704\n",
      "| epoch   1 |   350/ 2375 batches | accuracy    0.630\n",
      "| epoch   1 |   400/ 2375 batches | accuracy    0.750\n",
      "| epoch   1 |   450/ 2375 batches | accuracy    0.708\n",
      "| epoch   1 |   500/ 2375 batches | accuracy    0.782\n",
      "| epoch   1 |   550/ 2375 batches | accuracy    0.768\n",
      "| epoch   1 |   600/ 2375 batches | accuracy    0.776\n",
      "| epoch   1 |   650/ 2375 batches | accuracy    0.770\n",
      "| epoch   1 |   700/ 2375 batches | accuracy    0.648\n",
      "| epoch   1 |   750/ 2375 batches | accuracy    0.700\n",
      "| epoch   1 |   800/ 2375 batches | accuracy    0.770\n",
      "| epoch   1 |   850/ 2375 batches | accuracy    0.762\n",
      "| epoch   1 |   900/ 2375 batches | accuracy    0.744\n",
      "| epoch   1 |   950/ 2375 batches | accuracy    0.754\n",
      "| epoch   1 |  1000/ 2375 batches | accuracy    0.750\n",
      "| epoch   1 |  1050/ 2375 batches | accuracy    0.772\n",
      "| epoch   1 |  1100/ 2375 batches | accuracy    0.750\n",
      "| epoch   1 |  1150/ 2375 batches | accuracy    0.792\n",
      "| epoch   1 |  1200/ 2375 batches | accuracy    0.718\n",
      "| epoch   1 |  1250/ 2375 batches | accuracy    0.736\n",
      "| epoch   1 |  1300/ 2375 batches | accuracy    0.798\n",
      "| epoch   1 |  1350/ 2375 batches | accuracy    0.780\n",
      "| epoch   1 |  1400/ 2375 batches | accuracy    0.766\n",
      "| epoch   1 |  1450/ 2375 batches | accuracy    0.790\n",
      "| epoch   1 |  1500/ 2375 batches | accuracy    0.752\n",
      "| epoch   1 |  1550/ 2375 batches | accuracy    0.774\n",
      "| epoch   1 |  1600/ 2375 batches | accuracy    0.800\n",
      "| epoch   1 |  1650/ 2375 batches | accuracy    0.802\n",
      "| epoch   1 |  1700/ 2375 batches | accuracy    0.696\n",
      "| epoch   1 |  1750/ 2375 batches | accuracy    0.788\n",
      "| epoch   1 |  1800/ 2375 batches | accuracy    0.812\n",
      "| epoch   1 |  1850/ 2375 batches | accuracy    0.804\n",
      "| epoch   1 |  1900/ 2375 batches | accuracy    0.812\n",
      "| epoch   1 |  1950/ 2375 batches | accuracy    0.760\n",
      "| epoch   1 |  2000/ 2375 batches | accuracy    0.788\n",
      "| epoch   1 |  2050/ 2375 batches | accuracy    0.808\n",
      "| epoch   1 |  2100/ 2375 batches | accuracy    0.772\n",
      "| epoch   1 |  2150/ 2375 batches | accuracy    0.828\n",
      "| epoch   1 |  2200/ 2375 batches | accuracy    0.824\n",
      "| epoch   1 |  2250/ 2375 batches | accuracy    0.846\n",
      "| epoch   1 |  2300/ 2375 batches | accuracy    0.804\n",
      "| epoch   1 |  2350/ 2375 batches | accuracy    0.814\n",
      "| epoch   1 |    50/  125 batches | accuracy    0.827\n",
      "| epoch   1 |   100/  125 batches | accuracy    0.846\n",
      "epoch: 2\n",
      "train_loss: 0.505, train_acc: 0.759\n",
      "eval_loss: 0.367, eval_acc: 0.837\n"
     ]
    }
   ],
   "source": [
    "best_eval_loss = float('inf')\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "eval_loss_list = []\n",
    "eval_acc_list = []\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    train_losses, train_accs = train(train_dataloader, model, criterion, optimizer, device)\n",
    "    eval_losses, eval_accs = test(eval_dataloader, model, criterion, device)\n",
    "    train_loss_list.extend(train_losses)\n",
    "    train_acc_list.extend(train_accs)\n",
    "    eval_loss_list.extend(eval_losses)\n",
    "    eval_acc_list.extend(eval_accs) \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_train_acc = np.mean(train_accs)\n",
    "    epoch_eval_loss = np.mean(eval_losses)\n",
    "    epoch_eval_acc = np.mean(eval_accs)    \n",
    "    if epoch_eval_loss < best_eval_loss:\n",
    "        best_eval_loss = epoch_eval_loss\n",
    "        torch.save(model.state_dict(), 'model/lstm/BiLSTM.pt')   \n",
    "    print(f'epoch: {epoch + 1}')\n",
    "    print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
    "    print(f'eval_loss: {epoch_eval_loss:.3f}, eval_acc: {epoch_eval_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097081cb-064f-46ee-bf5f-0d864487f0d7",
   "metadata": {},
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21885765-51d8-4efe-9211-05b311cc3169",
   "metadata": {},
   "source": [
    "## 测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f831d2e-d5c0-4c2a-9911-7e14ebad8b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/ 2500 batches | accuracy    0.806\n",
      "| epoch   1 |   100/ 2500 batches | accuracy    0.832\n",
      "| epoch   1 |   150/ 2500 batches | accuracy    0.848\n",
      "| epoch   1 |   200/ 2500 batches | accuracy    0.820\n",
      "| epoch   1 |   250/ 2500 batches | accuracy    0.824\n",
      "| epoch   1 |   300/ 2500 batches | accuracy    0.822\n",
      "| epoch   1 |   350/ 2500 batches | accuracy    0.816\n",
      "| epoch   1 |   400/ 2500 batches | accuracy    0.834\n",
      "| epoch   1 |   450/ 2500 batches | accuracy    0.840\n",
      "| epoch   1 |   500/ 2500 batches | accuracy    0.844\n",
      "| epoch   1 |   550/ 2500 batches | accuracy    0.830\n",
      "| epoch   1 |   600/ 2500 batches | accuracy    0.844\n",
      "| epoch   1 |   650/ 2500 batches | accuracy    0.846\n",
      "| epoch   1 |   700/ 2500 batches | accuracy    0.832\n",
      "| epoch   1 |   750/ 2500 batches | accuracy    0.830\n",
      "| epoch   1 |   800/ 2500 batches | accuracy    0.800\n",
      "| epoch   1 |   850/ 2500 batches | accuracy    0.848\n",
      "| epoch   1 |   900/ 2500 batches | accuracy    0.828\n",
      "| epoch   1 |   950/ 2500 batches | accuracy    0.838\n",
      "| epoch   1 |  1000/ 2500 batches | accuracy    0.864\n",
      "| epoch   1 |  1050/ 2500 batches | accuracy    0.864\n",
      "| epoch   1 |  1100/ 2500 batches | accuracy    0.802\n",
      "| epoch   1 |  1150/ 2500 batches | accuracy    0.850\n",
      "| epoch   1 |  1200/ 2500 batches | accuracy    0.832\n",
      "| epoch   1 |  1250/ 2500 batches | accuracy    0.798\n",
      "| epoch   1 |  1300/ 2500 batches | accuracy    0.836\n",
      "| epoch   1 |  1350/ 2500 batches | accuracy    0.812\n",
      "| epoch   1 |  1400/ 2500 batches | accuracy    0.846\n",
      "| epoch   1 |  1450/ 2500 batches | accuracy    0.844\n",
      "| epoch   1 |  1500/ 2500 batches | accuracy    0.830\n",
      "| epoch   1 |  1550/ 2500 batches | accuracy    0.852\n",
      "| epoch   1 |  1600/ 2500 batches | accuracy    0.866\n",
      "| epoch   1 |  1650/ 2500 batches | accuracy    0.842\n",
      "| epoch   1 |  1700/ 2500 batches | accuracy    0.840\n",
      "| epoch   1 |  1750/ 2500 batches | accuracy    0.814\n",
      "| epoch   1 |  1800/ 2500 batches | accuracy    0.818\n",
      "| epoch   1 |  1850/ 2500 batches | accuracy    0.806\n",
      "| epoch   1 |  1900/ 2500 batches | accuracy    0.834\n",
      "| epoch   1 |  1950/ 2500 batches | accuracy    0.822\n",
      "| epoch   1 |  2000/ 2500 batches | accuracy    0.820\n",
      "| epoch   1 |  2050/ 2500 batches | accuracy    0.832\n",
      "| epoch   1 |  2100/ 2500 batches | accuracy    0.854\n",
      "| epoch   1 |  2150/ 2500 batches | accuracy    0.840\n",
      "| epoch   1 |  2200/ 2500 batches | accuracy    0.796\n",
      "| epoch   1 |  2250/ 2500 batches | accuracy    0.796\n",
      "| epoch   1 |  2300/ 2500 batches | accuracy    0.870\n",
      "| epoch   1 |  2350/ 2500 batches | accuracy    0.832\n",
      "| epoch   1 |  2400/ 2500 batches | accuracy    0.858\n",
      "| epoch   1 |  2450/ 2500 batches | accuracy    0.818\n",
      "test_loss: 0.382, test_acc: 0.832\n"
     ]
    }
   ],
   "source": [
    "# 加载model\n",
    "model.load_state_dict(torch.load(\"model/lstm/BiLSTM.pt\"))\n",
    "test_losses, test_accs = test(test_dataloader, model, criterion, device)\n",
    "print(f'test_loss: {np.mean(test_losses):.3f}, test_acc: {np.mean(test_accs):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b40f53-0598-4875-8ba8-8ee443776048",
   "metadata": {},
   "source": [
    "## 真实影评测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74f1698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        processed_text = text_pipeline(text)[:max_length]\n",
    "        length = torch.tensor([len(processed_text)], dtype=torch.int64)\n",
    "        text = torch.tensor([(processed_text + pad * max_length)[:max_length]])\n",
    "        output = model(text, length)\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55baa58f-f3b3-4c40-87fc-e112c7f697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_label_dict = {1: \"neg\",\n",
    "                     2: \"pos\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a4fda29-8bea-4625-9db3-4ffb4650469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dd540db-f3bd-491d-84ab-311beafda8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a neg review\n"
     ]
    }
   ],
   "source": [
    "review1 = \"This movie is too rubbish, and I don't like it very much.\"\n",
    "print(\"This is a %s review\" % review_label_dict[predict(review1, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee222d5-b274-4673-8023-56b225785594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a pos review\n"
     ]
    }
   ],
   "source": [
    "review2 = \"this movie is great, and I love it very much.\"\n",
    "print(\"This is a %s review\" % review_label_dict[predict(review2, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a64d40-a863-4fde-ae0c-cac6855a51a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "181.506px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
