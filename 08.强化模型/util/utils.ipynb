{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96c9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gym\n",
    "import collections\n",
    "import random\n",
    "from IPython import display\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd0eb2-7dd3-49c9-9bac-12d262847f55",
   "metadata": {},
   "source": [
    "# 环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb2914-e435-4abf-95c3-91b3ee2df48a",
   "metadata": {},
   "source": [
    "## 显示所有Gym环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcba9e2-8636-4e59-bdda-29bdac2014f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAllGymEnvs():\n",
    "    for env in gym.envs.registry:\n",
    "        print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae667e4-ec32-4da9-8525-e8a482ebe2a5",
   "metadata": {},
   "source": [
    "## 离散动作环境（动作取值离散）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602301df-4876-4268-8b3a-762ed6af7333",
   "metadata": {},
   "source": [
    "### 悬崖漫步环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5d6d23-2161-4fab-bf62-3d091f42cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv(gym.Wrapper):\n",
    "    '''\n",
    "    ‌悬崖漫步环境‌是一个经典的强化学习环境，\n",
    "    用于训练智能体在避免掉入悬崖的情况下从起点移动到终点。\n",
    "    该环境由一个4x12的网格组成，智能体的起点位于左下角，目标点位于右下角。\n",
    "    智能体可以执行上、下、左、右四个动作（0 up, 1 right, 2 down, 3 left），\n",
    "    每走一步会获得-1的奖励，如果掉入悬崖或到达终点，则游戏结束。\n",
    "    悬崖漫步环境的特性\n",
    "    ‌网格世界‌：环境由4行12列的网格组成，每个网格代表一个状态。\n",
    "    ‌起始和终止条件‌：智能体从左下角开始，目标是到达右下角。\n",
    "    掉入悬崖或到达终点时，游戏结束。\n",
    "    ‌动作空间‌：智能体可以执行上、下、左、右四个动作。\n",
    "    ‌奖励机制‌：每走一步获得-1的奖励，掉入悬崖获得-100的奖励。\n",
    "    悬崖漫步环境的应用场景\n",
    "    悬崖漫步环境常用于测试和训练强化学习算法，\n",
    "    特别是用于策略迭代和动态规划算法的示例。\n",
    "    通过这个环境，研究者可以测试不同算法在面对障碍和终点时的表现，\n",
    "    从而优化智能体的决策过程。\n",
    "    '''\n",
    "    name = 'CliffWalking-v0'\n",
    "    def __init__(self, max_step=None, failure_score=None):\n",
    "        env = gym.make(self.name, render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "        # self.max_step = 200\n",
    "        self.max_step = max_step\n",
    "        self.failure_score = failure_score\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.step_n = 0\n",
    "        if seed:\n",
    "            state, _ = self.env.reset(seed = seed)\n",
    "        else:\n",
    "            state, _ = self.env.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # 参数是一个离散值（0或1），表示向左或向右移动。\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.step_n += 1\n",
    "        if self.max_step:\n",
    "            # 限制最大步数\n",
    "            if self.step_n >= self.max_step:\n",
    "                done = True\n",
    "        if self.failure_score:\n",
    "            # 没坚持到最后，扣分\n",
    "            if done and self.step_n < self.max_step:\n",
    "                # reward = -1000\n",
    "                reward = self.failure_score\n",
    "        return state, reward, done\n",
    "\n",
    "    # 打印游戏图像\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c005a20-abf1-4d0a-8f05-90061d3f2f16",
   "metadata": {},
   "source": [
    "### 悬崖漫步环境（自定义）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a73fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCliffWalkingEnv(gym.Env):\n",
    "    '''\n",
    "    ‌悬崖漫步环境‌是一个经典的强化学习环境，\n",
    "    用于训练智能体在避免掉入悬崖的情况下从起点移动到终点。\n",
    "    该环境由一个4x12的网格组成，智能体的起点位于左下角，目标点位于右下角。\n",
    "    智能体可以执行上、下、左、右四个动作（0 up, 1 right, 2 down, 3 left），\n",
    "    每走一步会获得-1的奖励，如果掉入悬崖或到达终点，则游戏结束。\n",
    "    悬崖漫步环境的特性\n",
    "    ‌网格世界‌：环境由4行12列的网格组成，每个网格代表一个状态。\n",
    "    ‌起始和终止条件‌：智能体从左下角开始，目标是到达右下角。\n",
    "    掉入悬崖或到达终点时，游戏结束。\n",
    "    ‌动作空间‌：智能体可以执行上、下、左、右四个动作。\n",
    "    ‌奖励机制‌：每走一步获得-1的奖励，掉入悬崖获得-100的奖励。\n",
    "    悬崖漫步环境的应用场景\n",
    "    悬崖漫步环境常用于测试和训练强化学习算法，\n",
    "    特别是用于策略迭代和动态规划算法的示例。\n",
    "    通过这个环境，研究者可以测试不同算法在面对障碍和终点时的表现，\n",
    "    从而优化智能体的决策过程。\n",
    "    '''\n",
    "    name = 'MyCliffWalkingEnv'\n",
    "    action_meaning = ['^', 'v', '<', '>']\n",
    "    def __init__(self, ncol, nrow, disaster=list(range(37, 47)), end=[47]):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "        self.disaster = disaster\n",
    "        self.end = end\n",
    "        self.action_list = []\n",
    "        self.state_list = []\n",
    "\n",
    "    def reset(self):  # 回归初始状态（左下角）\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        self.action_list = []\n",
    "        self.state_list = []\n",
    "        return self.y * self.ncol + self.x\n",
    "     \n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        self.action_list.append(action)\n",
    "        # 4种动作，change[0]:上，change[1]:下，change[2]:左，change[3]:右。\n",
    "        # 坐标系原点(0, 0)定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        self.state_list.append((self.y, self.x))\n",
    "        reward = -1\n",
    "        done = False\n",
    "        # 下一个位置在悬崖或者目标\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  \n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def show(self):\n",
    "        print(\"动作总数：{}\".format(len(self.action_list)))\n",
    "        if len(self.action_list) > 20:\n",
    "            print(\"前20个动作和对应的状态：\")\n",
    "        else:\n",
    "            print(\"动作和对应的状态：\")\n",
    "        for action, state in list(zip(self.action_list, self.state_list))[:20]:\n",
    "            print(\"{}->{}\".format(action, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393bbbc-3b22-47c5-8d15-ab554541da6e",
   "metadata": {},
   "source": [
    "### 车杆平衡环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45467a08-a8c7-4fbd-a5d7-8b49ab6a3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv(gym.Wrapper):\n",
    "    '''\n",
    "    ‌Gym中的CartPole环境是离散动作空间‌。在CartPole环境中，\n",
    "    小车可以通过两个离散动作来控制：向左移动或向右移动。具体来说，动作空间包括：\n",
    "    ‌0‌：小车向左移动\n",
    "    ‌1‌：小车向右移动\n",
    "    这些动作是固定的，施加的力大小也是固定的，\n",
    "    但速度和位移会根据杆子与竖直方向的角度变化而有所不同。\n",
    "    此外，CartPole环境的观测值包括：\n",
    "    ‌x‌：小车位置（范围：[-4.8, 4.8]）\n",
    "    ‌x˙‌：小车速度（范围：−∞ 到 ∞∞）\n",
    "    ‌θ‌：杆子角度（范围：[−24度，24度]）\n",
    "    ‌θ˙‌：杆子顶端速度（范围：−∞ 到 ∞）。\n",
    "    '''\n",
    "    name = 'CartPole-v1'\n",
    "    def __init__(self, max_step=None, failure_score=None):\n",
    "        env = gym.make(self.name, render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "        # self.max_step = 200\n",
    "        self.max_step = max_step\n",
    "        self.failure_score = failure_score\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.step_n = 0\n",
    "        if seed:\n",
    "            state, _ = self.env.reset(seed = seed)\n",
    "        else:\n",
    "            state, _ = self.env.reset()\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # 参数是一个离散值（0或1），表示向左或向右移动。\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.step_n += 1\n",
    "        if self.max_step:\n",
    "            # 限制最大步数\n",
    "            if self.step_n >= self.max_step:\n",
    "                done = True\n",
    "        if self.failure_score:\n",
    "            # 没坚持到最后，扣分\n",
    "            if done and self.step_n < self.max_step:\n",
    "                # reward = -1000\n",
    "                reward = self.failure_score\n",
    "        return state, reward, done\n",
    "\n",
    "    # 打印游戏图像\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b351982-2df7-45dd-b8c7-dce70a536010",
   "metadata": {},
   "source": [
    "## 连续动作环境（动作取值连续）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa53da3-cfd0-4d41-9a12-8110eb2ddfb4",
   "metadata": {},
   "source": [
    "### 倒立摆环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "610f21f2-d595-493e-98e7-e6fa17e52855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendulumEnv(gym.Wrapper):\n",
    "    '''\n",
    "    ‌Gym中的Pendulum环境是连续动作空间。\n",
    "    智能体需要通过施加一个连续的力矩来控制倒立摆的摆动，使其保持竖直状态。\n",
    "    Pendulum环境的动作空间和状态空间如下：\n",
    "    动作空间 (action_space)：一个一维向量，表示施加在摆杆上的扭矩，范围为[-2, 2]（牛顿·米）。\n",
    "    状态空间 (observation_space)：一个三维向量，包含以下信息：\n",
    "        theta_dot：摆杆的角速度（弧度/秒），范围为 [-8, 8]。\n",
    "        cos(theta) 和 sin(theta)：这两个值通常用于表示角度，避免角度值的不连续性。\n",
    "    其中，theta为摆杆的角度（弧度），范围为 [-π, π]。\n",
    "    通过训练，智能体需要学习如何通过施加合适的力矩来最小化负奖励，\n",
    "    从而使倒立摆保持竖直状态。\n",
    "    注：\n",
    "    Pendulum-v0环境的动作空间是从-2到2的一个连续值，表示施加在摆杆上的一个扭矩。\n",
    "    Pendulum-v1环境的动作空间是从-2到2的一个连续值一维向量，表示施加在摆杆上的若干个扭矩。\n",
    "    '''\n",
    "    name = 'Pendulum-v1'\n",
    "    def __init__(self, max_step=200):\n",
    "        env = gym.make(self.name, render_mode='rgb_array')\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.step_n = 0\n",
    "        # self.max_step = 200\n",
    "        self.max_step = max_step\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed:\n",
    "            state, _ = self.env.reset(seed = seed)\n",
    "        else:\n",
    "            state, _ = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # 参数是从-2到2的一个连续值一维向量，表示施加在摆杆上的若干个扭矩。\n",
    "        state, reward, terminated, truncated, info = self.env.step(action)        \n",
    "        done = terminated or truncated\n",
    "        # 限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.max_step:\n",
    "            if self.step_n >= self.max_step:\n",
    "                done = True\n",
    "        # 对倒立摆环境的奖励进行修改以便训练\n",
    "        reward = (reward + 8.0) / 8.0  \n",
    "        return state, reward, done\n",
    "\n",
    "    # 打印游戏图像\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(self.env.render())\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179906dd-636d-4642-ad70-029bbd9adcb0",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d896be-c076-419c-bb86-7a4ae44e35e6",
   "metadata": {},
   "source": [
    "## 训练结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4de37-5e62-4e4b-9924-531de2740661",
   "metadata": {},
   "source": [
    "### 显示回报"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a6bc86-3ef2-4362-a1ce-aba048997ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_return(return_list, agent, env):\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list, return_list)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.title('{} on {}'.format(agent.name, env.name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce7dfe-446d-42ac-b822-b5f646985996",
   "metadata": {},
   "source": [
    "### 移动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d660be2a-48ee-4861-b027-20ac7b8cf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(li, window_size):\n",
    "    cumulative_sum = np.cumsum(np.insert(li, 0, 0)) \n",
    "    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n",
    "    r = np.arange(1, window_size-1, 2)\n",
    "    begin = np.cumsum(li[:window_size-1])[::2] / r\n",
    "    end = (np.cumsum(li[:-window_size:-1])[::2] / r)[::-1]\n",
    "    return np.concatenate((begin, middle, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e905f22-9a73-45c6-bba8-30af063adde5",
   "metadata": {},
   "source": [
    "# 经验回放池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48b14b0-5255-478d-9764-dfc6ecb3ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, buffer_size):\n",
    "        # deque代表“double-ended queue”，即双端队列\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)  # 队列，先进先出\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据，数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d9843-93e3-418f-9f5f-9071f4dcaf1f",
   "metadata": {},
   "source": [
    "# 计算广义优势估计(GAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b4935-5ec4-4492-801c-31df37aa5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Advantage Estimation\n",
    "def compute_gae(gamma, lmbda, td_delta):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0.0\n",
    "    for delta in td_delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    return torch.tensor(advantage_list, dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
